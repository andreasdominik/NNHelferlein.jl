<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · NNHelferlein.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="NNHelferlein.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">NNHelferlein.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li><a class="tocitem" href="#Fully-connected-layers"><span>Fully connected layers</span></a></li><li><a class="tocitem" href="#Convolutional"><span>Convolutional</span></a></li><li><a class="tocitem" href="#Recurrent"><span>Recurrent</span></a></li><li><a class="tocitem" href="#Others"><span>Others</span></a></li><li><a class="tocitem" href="#Attention-Mechanisms"><span>Attention Mechanisms</span></a></li><li><a class="tocitem" href="#Layers-for-transformers"><span>Layers for transformers</span></a></li><li class="toplevel"><a class="tocitem" href="#Data-providers"><span>Data providers</span></a></li><li><a class="tocitem" href="#Tabular-data"><span>Tabular data</span></a></li><li><a class="tocitem" href="#Image-data"><span>Image data</span></a></li><li><a class="tocitem" href="#Text-data"><span>Text data</span></a></li><li class="toplevel"><a class="tocitem" href="#Training"><span>Training</span></a></li><li class="toplevel"><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li class="toplevel"><a class="tocitem" href="#ImageNet-tools"><span>ImageNet tools</span></a></li><li class="toplevel"><a class="tocitem" href="#Other-utils"><span>Other utils</span></a></li><li><a class="tocitem" href="#Utils-for-transformers"><span>Utils for transformers</span></a></li><li><a class="tocitem" href="#Utils-for-array-manipulation"><span>Utils for array manipulation</span></a></li><li><a class="tocitem" href="#Utils-for-fixing-types-in-GPU-context"><span>Utils for fixing types in GPU context</span></a></li></ul></li><li><a class="tocitem" href="../license/">License</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>API doc of all exported functions are listed here:</p><h1 id="Chains"><a class="docs-heading-anchor" href="#Chains">Chains</a><a id="Chains-1"></a><a class="docs-heading-anchor-permalink" href="#Chains" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DNN" href="#NNHelferlein.DNN"><code>NNHelferlein.DNN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type DNN end</code></pre><p>Mother type for DNN hierarchy with implementation for a chain of layers.</p><p><strong>Signatures:</strong></p><pre><code class="language-Julia hljs">(m::DNN)(x) = (for l in m.layers; x = l(x); end; x)
(m::DNN)(x,y) = m(x,y)
(m::DNN)(d::Knet.Data) = mean( m(x,y) for (x,y) in d)
(m::DNN)(d::Tuple) = mean( m(x,y) for (x,y) in d)
(m::DNN)(d::NNHelferlein.DataLoader) = mean( m(x,y) for (x,y) in d)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L6-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.add_layer!" href="#NNHelferlein.add_layer!"><code>NNHelferlein.add_layer!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">add_layer!(n::NNHelferlein.DNN, l)</code></pre><p>Add a layer <code>l</code> or a chain to a model <code>n</code>. The layer is always added  at the end of the chains. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L79-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.print_network" href="#NNHelferlein.print_network"><code>NNHelferlein.print_network</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function print_network(mdl::DNN)</code></pre><p>Print a network summary of any model of Type <code>DNN</code>. If the model has a field <code>layers</code>, the summary of all included layers will be printed recursively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L103-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Classifier" href="#NNHelferlein.Classifier"><code>NNHelferlein.Classifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Classifier &lt;: DNN</code></pre><p>Classifier with nll loss.</p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(m::Classifier)(x,y) = nll(m(x), y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L28-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Regressor" href="#NNHelferlein.Regressor"><code>NNHelferlein.Regressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Regressor</code></pre><p>Regression network with square loss.</p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(m::Regression)(x,y) = sum(abs2.( m(x) - y))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L46-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Chain" href="#NNHelferlein.Chain"><code>NNHelferlein.Chain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Chain</code></pre><p>Simple wrapper to chain layers and execute them one after another.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.VAE" href="#NNHelferlein.VAE"><code>NNHelferlein.VAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct VAE</code></pre><p>Type for a generic variational autoencoder.</p><p><strong>Constructor:</strong></p><pre><code class="nohighlight hljs">VAE(encoder, decoder)</code></pre><p>Separate predefinded chains (ideally, but not necessarily of type <code>Chain</code>)  for encoder and decoder must be specified. The VAE needs the 2 parameters mean and variance to define the distribution of each code-neuron in the bottleneck-layer. In consequence the encoder outputmust be 2 times  the size of the decoder input (in case of dense layers: if encoder output is a 8-value vector, 4 codes are defined and the decoder input is a 4-value vector; in case of convolutional layers the number of encoder output channels must be 2 times the number of the encoder input channels - see the examples). </p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">(vae::VAE)(x)
(vae::VAE)(x,y)</code></pre><p>Called with one argument, predict will be executed;  with two arguments (args x and y should be identical for the autoencoder) the loss will be returned.    </p><p><strong>Details:</strong></p><p>The loss is calculated as the sum of element-wise error squares plus the <em>Kullback-Leibler-Divergence</em> to adapt the distributions of the bottleneck codes:</p><p class="math-container">\[\mathcal{L} = \frac{1}{2} \sum_{i=1}^{n_{outputs}} (t_{i}-o_{i})^{2} - 
               \frac{1}{2} \sum_{j=1}^{n_{codes}}(1 + ln\sigma_{c_j}^{2}-\mu_{c_j}^{2}-\sigma_{c_j}^{2}) \]</p><p>Output of the autoencoder is cropped to the size of input before loss calculation (and before prediction); i.e. the output has always the same dimensions as the input, even if the last layer generates a bigger shape.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/nets.jl#L176-L213">source</a></section></article><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Layer" href="#NNHelferlein.Layer"><code>NNHelferlein.Layer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type Layer end</code></pre><p>Mother type for layers hierarchy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L7-L11">source</a></section></article><h2 id="Fully-connected-layers"><a class="docs-heading-anchor" href="#Fully-connected-layers">Fully connected layers</a><a id="Fully-connected-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Fully-connected-layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dense" href="#NNHelferlein.Dense"><code>NNHelferlein.Dense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Dense  &lt;: Layer</code></pre><p>Default Dense layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Dense(w, b, actf)</code>: default constructor</li><li><code>Dense(i::Int, j::Int; actf=sigm)</code>: layer of j neurons with       i inputs.</li><li><code>Dense(h5::HDF5.File, group::String; trainable=false, actf=sigm)</code>:</li><li><code>Dense(h5::HDF5.File, kernel::String, bias::String;       trainable=false, actf=sigm)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L16-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Linear" href="#NNHelferlein.Linear"><code>NNHelferlein.Linear</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Linear  &lt;: Layer</code></pre><p>Almost standard dense layer, but functionality inspired by the TensorFlow-layer:</p><ul><li>capable to work with input tensors of any number of dimensions</li><li>default activation function <code>indetity</code></li><li>optionally without biases.</li></ul><p>The shape of the input tensor is preserved; only the size of the first dim is changed from in to out.</p><p><strong>Constructors:</strong></p><ul><li><code>Linear(i::Int, j::Int; bias=true, actf=identity)</code> weher <code>i</code> is fan-in       and <code>j</code> is fan-out.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>bias=true</code>: if false biases are fixed to 0.0</li><li><code>actf=identity</code>: activation function.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L70-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Embed" href="#NNHelferlein.Embed"><code>NNHelferlein.Embed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Embed &lt;: Layer</code></pre><p>Simple type for an embedding layer to embed a virtual onehot-vector into a smaller number of neurons by linear combination. The onehot-vector is virtual, because not the vector, but only the index of the &quot;one&quot; in the vector has to be provided as Integer value (or a minibatch of integers).</p><p><strong>Fields:</strong></p><ul><li>w</li><li>actf</li></ul><p><strong>Constructors:</strong></p><ul><li><code>Embed(v,d; actf=identity):</code> with   vocab size v, embedding depth d and default activation function idendity.</li></ul><p><strong>Signatures:</strong></p><ul><li><code>(l::Embed)(x) = l.actf.(w[:,x])</code> default embedding of input tensor x.</li></ul><p><strong>Value:</strong></p><p>The embedding is constructed by adding a first dimension to the input tensor with number of rows = embedding depth. If x is a column vector, the value is a matrix. If x is as row-vector or a matrix, the value is a 3-d array, etc.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L364-L390">source</a></section></article><h2 id="Convolutional"><a class="docs-heading-anchor" href="#Convolutional">Convolutional</a><a id="Convolutional-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Conv" href="#NNHelferlein.Conv"><code>NNHelferlein.Conv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Conv  &lt;: Layer</code></pre><p>Default Conv layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Conv(w, b, padding, actf)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu; kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i layers.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>dilation=1</code>: dilation factor for each dimension.</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>conv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L118-L140">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DeConv" href="#NNHelferlein.DeConv"><code>NNHelferlein.DeConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct DeConv  &lt;: Layer</code></pre><p>Default deconvolution layer.</p><p><strong>Constructors:</strong></p><ul><li><code>DeConv(w, b, actf, kwargs...)</code>: default constructor</li><li><code>Conv(w1::Int, w2::Int,  i::Int, o::Int; actf=relu, kwargs...)</code>: layer with   o kernels of size (w1,w2) for an input of i channels.</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>:</li><li><code>Conv(h5::HDF5.File, group::String; trainable=false, actf=relu)</code>: layer       imported from a hdf5-file from tensorflow with the       hdf-object hdfo and the group name group.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>padding=0</code>: the number of extra zeros implicitly concatenated       at the start and end of each dimension (applied to the output).</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window       (applied to the output).</li><li><code>...</code> See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>deconv4()</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L234-L257">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Pool" href="#NNHelferlein.Pool"><code>NNHelferlein.Pool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Pool &lt;: Layer</code></pre><p>Pooling layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Pool(;kwargs...)</code>: max pooling; without kwargs, 2x2-pooling       is performed.</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>window=2</code>: pooling window size (same for both directions)</li><li><code>...</code>: See the Knet documentation for Details:       https://denizyuret.github.io/Knet.jl/latest/reference/#Convolution-and-Pooling.       All keywords to the Knet function <code>pool</code> are supported.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L195-L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.UnPool" href="#NNHelferlein.UnPool"><code>NNHelferlein.UnPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct UnPool &lt;: Layer</code></pre><p>Unpooling layer.</p><p><strong>Constructors:</strong></p><ul><li><code>UnPool(;kwargs...)</code>: user-defined unpooling</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L286-L293">source</a></section></article><h2 id="Recurrent"><a class="docs-heading-anchor" href="#Recurrent">Recurrent</a><a id="Recurrent-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.RecurrentUnit" href="#NNHelferlein.RecurrentUnit"><code>NNHelferlein.RecurrentUnit</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type RecurrentUnit end</code></pre><p>Supertype for all recurrent unit types. Self-defined recurrent units which are a child of <code>RecurrentUnit</code> can be used inside the &#39;Recurrent&#39; layer.</p><p><strong>Interface</strong></p><p>All subtypes of <code>RecurrentUnit</code> must provide the followning:</p><ul><li>a constructor with signature <code>Type(n_inputs, n_units; kwargs)</code> and   arbitrary keyword arguments.</li><li>an implementation of signature <code>(o::Recurrent)(x)</code>   where <code>x</code> is a 3d- or 2d-array of shape [fan-in, mb-size, 1] or    [fan-in, mb-size].   The function must return the result of one forward    computation for one step and return the hidden state   and set the internal fields <code>h</code> and optionally <code>c</code>.</li><li>a field <code>h</code> (to store the last hidden state)</li><li>an optional field <code>c</code>, if the cell state is to be stored   such as in a lstm unit.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/types.jl#L100-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Recurrent" href="#NNHelferlein.Recurrent"><code>NNHelferlein.Recurrent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Recurrent &lt;: Layer</code></pre><p>One layer RNN that works with minimatches of (time) series data. minibatch can be a 2- or 3-dimensional Array. If 2-d, inputs for one step are in one column and the Array has as many colums as steps. If 3-d, the last dimension iterates the samples of the minibatch.</p><p>Result is an array matrix with the output of the units of all steps for all smaples of the minibatch (with model depth as first and samples of the minimatch as last dimension).</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">Recurrent(n_inputs::Int, n_units::Int; u_type=:lstm, 
          bidirectional=false, allow_mask=false, o...)</code></pre><ul><li><code>n_inputs</code>: number of inputs</li><li><code>n_units</code>:  number of units </li><li><code>u_type</code> :  unit type can be one of the Knet unit types       (<code>:relu, :tanh, :lstm, :gru</code>) or a type which must be a        subtype of <code>RecurrentUnit</code> and fullfill the repective interface        (see the docs for <code>RecurentUnit</code>).</li><li><code>bidirectional=false</code>: if true, 2 layers of <code>n_units</code> units will be defined       and run in forward and backward direction respectively. The hidden       state is <code>[2*n_units*mb]</code> or <code>[2*n_units,steps,mb]</code> id <code>return_all==true</code>.</li><li><code>allow_mask=false</code>: if maskin is allowed a slower algorithm is used to be        able to ignore any masked step. Arbitrary sequence positions may be        masked for any sequence.</li></ul><p>Any keyword argument of <code>Knet.RNN</code> or  a self-defined <code>RecurrentUnit</code> type may be provided.</p><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">function (rnn::Recurrent)(x; c=nothing, h=nothing, return_all=false, 
          mask=nothing)</code></pre><p>The layer is called either with a 2-dimensional array of the shape [fan-in, steps]  or a 3-dimensional array of [fan-in, steps, batchsize].</p><p><strong>Arguments:</strong></p><ul><li><code>c=nothing</code>, <code>h=nothing</code>: inits the hidden and cell state.   If <code>nothing</code>,  states <code>h</code> or <code>c</code> keep their values.    If <code>c=0</code> or <code>h=0</code>, the states are reseted to <code>0</code>;   otherwise an array of states of the correct dimensions can be supplied    to be used as initial states.</li><li><code>return_all=true</code>: if <code>true</code> an array with all hidden states of all steps    is returned (size is [units, time-steps, minibatch]).   Otherwise only the hidden states of the last step is returned   ([units, minibatch]).</li><li><code>mask</code>: optional mask for the input sequence minibatch of shape    [steps, minibatch]. Values in the mask must be 1.0 for masked positions   or 0.0 otherwise and of type <code>Float32</code> or <code>CuArray{Float32}</code> for GPU context.    Appropriate masks can be generated with the NNHelferlein function    <code>mk_padding_mask()</code>.</li></ul><p>Bidirectional layers can be constructed by specifying <code>bidirectional=true</code>, if the unit-type supports it (Knet.RNN does.).  Please be aware that the actual number of units is 2<em>n_units for  bidirectional layers and the output dimension is [2</em>units, steps, mb] or [2*units, mb].</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L599-L662">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_hidden_states" href="#NNHelferlein.get_hidden_states"><code>NNHelferlein.get_hidden_states</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_hidden_states(l::&lt;RNN_Type&gt;; flatten=true)</code></pre><p>Return the hidden states of one or more layers of an RNN. <code>&lt;RNN_Type&gt;</code> is one of <code>NNHelderlein.Recurrent</code>, <code>Knet.RNN</code>.</p><p><strong>Arguments:</strong></p><ul><li><code>flatten=true</code>: if the states tensor is 3d with a 3rd dim &gt; 1, the        array is transformed to [units, mb, 1] to represent all current states       after the last step.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L830-L840">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_cell_states" href="#NNHelferlein.get_cell_states"><code>NNHelferlein.get_cell_states</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_cell_states(l::&lt;RNN_Type&gt;; unbox=true, flatten=true)</code></pre><p>Return the cell states of one or more layers of an RNN only if it is a LSTM.</p><p><strong>Arguments:</strong></p><ul><li><code>unbox=true</code>: By default, c is unboxed when called in <code>@diff</code> context (while AutoGrad        is recording) to avoid unwanted dependencies of the computation graph       s2s.attn(reset=true)       (backprop should run via the hidden states, not the cell states).</li><li><code>flatten=true</code>: if the states tensor is 3d with a 3rd dim &gt; 1, the        array is transformed to [units, mb, 1] to represent all current states       after the last step.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L860-L874">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.set_hidden_states!" href="#NNHelferlein.set_hidden_states!"><code>NNHelferlein.set_hidden_states!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function set_hidden_states!(l::&lt;RNN_Type&gt;, h)</code></pre><p>Set the hidden states of one or more layers of an RNN to h.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L897-L902">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.set_cell_states!" href="#NNHelferlein.set_cell_states!"><code>NNHelferlein.set_cell_states!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function set_cell_states!(l::&lt;RNN_Type&gt;, c)</code></pre><p>Set the cell states of one or more layers of an RNN to c.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L911-L916">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.reset_hidden_states!" href="#NNHelferlein.reset_hidden_states!"><code>NNHelferlein.reset_hidden_states!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function reset_hidden_states!(l::&lt;RNN_Type&gt;)</code></pre><p>Reset the hidden states of one or more layers of an RNN to 0.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L927-L932">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.reset_cell_states!" href="#NNHelferlein.reset_cell_states!"><code>NNHelferlein.reset_cell_states!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function reset_cell_states!(l::&lt;RNN_Type&gt;)</code></pre><p>Reset the cell states of one or more layers of an RNN to 0.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L941-L946">source</a></section></article><h2 id="Others"><a class="docs-heading-anchor" href="#Others">Others</a><a id="Others-1"></a><a class="docs-heading-anchor-permalink" href="#Others" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Flat" href="#NNHelferlein.Flat"><code>NNHelferlein.Flat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Flat &lt;: Layer</code></pre><p>Default flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>Flat()</code>: with no options.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L314-L321">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PyFlat" href="#NNHelferlein.PyFlat"><code>NNHelferlein.PyFlat</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct PyFlat &lt;: Layer</code></pre><p>Flatten layer with optional Python-stype flattening (row-major). This layer can be used if pre-trained weight matrices from tensorflow are applied after the flatten layer.</p><p><strong>Constructors:</strong></p><ul><li><code>PyFlat(; python=true)</code>: if true, row-major flatten is performed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L336-L345">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Softmax" href="#NNHelferlein.Softmax"><code>NNHelferlein.Softmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Softmax &lt;: Layer</code></pre><p>Simple softmax layer to compute softmax probabilities.</p><p><strong>Constructors:</strong></p><ul><li><code>Softmax()</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L409-L416">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.Dropout" href="#NNHelferlein.Dropout"><code>NNHelferlein.Dropout</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct Dropout &lt;: Layer</code></pre><p>Dropout layer. Implemented with help of Knet&#39;s dropout() function that evaluates AutoGrad.recording() to detect if in training or inprediction. Dropouts are applied only if prediction.</p><p><strong>Constructors:</strong></p><ul><li><code>Dropout(p)</code> with the dropout rate <em>p</em>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L429-L439">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.BatchNorm" href="#NNHelferlein.BatchNorm"><code>NNHelferlein.BatchNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct BatchNorm &lt;: Layer</code></pre><p>Batchnormalisation layer. Implemented with help of Knet&#39;s batchnorm() function that evaluates AutoGrad.recording() to detect if in training or in prediction. In training the moments are updated to record the running averages; in prediction the moments are applied, but not modified.</p><p>In addition, optional trainable factor <code>a</code> and bias <code>b</code> are applied:</p><p class="math-container">\[y = a \cdot \frac{(x - \mu)}{(\sigma + \epsilon)} + b\]</p><p><strong>Constructors:</strong></p><ul><li><code>Batchnom(; trainable=false, channels=0)</code> will initialise       the moments with <code>Knet.bnmoments()</code> and       trainable parameters <code>a</code> and <code>b</code> only if       <code>trainable==true</code> (in this case, the number of channels must       be defined - for CNNs this is the number of feature maps).</li></ul><p><strong>Details:</strong></p><p>2d, 4d and 5d inputs are supported. Mean and variance are computed over dimensions (2), (1,2,4) and (1,2,3,5) for 2d, 4d and 5d arrays, respectively.</p><p>If <code>trainable=true</code> and <code>channels != 0</code>, trainable parameters <code>a</code> and <code>b</code> will be initialised for each channel.</p><p>If <code>trainable=true</code> and <code>channels == 0</code> (i.e. <code>Batchnom(trainable=true)</code>), the params <code>a</code> and <code>b</code> are not initialised by the constructor. Instead, the number of channels is inferred when the first minibatch is normalised as: 2d: <code>size(x)[1]</code> 4d: <code>size(x)[3]</code> 5d: <code>size(x)[4]</code> or <code>0</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L455-L493">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.LayerNorm" href="#NNHelferlein.LayerNorm"><code>NNHelferlein.LayerNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct LayerNorm  &lt;: Layer</code></pre><p>Simple layer normalisation (inspired by TFs LayerNormalization). Implementation is from Deniz Yuret&#39;s answer to feature request 429 (https://github.com/denizyuret/Knet.jl/issues/492).</p><p>The layer performs a normalisation within each sample, <em>not</em> batchwise. Normalisation is modified by two trainable parameters <code>a</code> and <code>b</code> (variance and mean) added to every value of the sample vector.</p><p><strong>Constructors:</strong></p><ul><li><code>LayertNorm(depth; eps=1e-6)</code>:  <code>depth</code> is the number       of activations for one sample of the layer.</li></ul><p><strong>Signatures:</strong></p><ul><li><code>function (l::LayerNorm)(x; dims=1)</code>: normalise x along the given dimensions.       The size of the specified dimension must fit with the initialised <code>depth</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/layers.jl#L551-L570">source</a></section></article><h2 id="Attention-Mechanisms"><a class="docs-heading-anchor" href="#Attention-Mechanisms">Attention Mechanisms</a><a id="Attention-Mechanisms-1"></a><a class="docs-heading-anchor-permalink" href="#Attention-Mechanisms" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttentionMechanism" href="#NNHelferlein.AttentionMechanism"><code>NNHelferlein.AttentionMechanism</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type AttentionMechanism</code></pre><p>Attention mechanisms follow the same interface and common signatures.</p><p>If possible, the algorithm allows precomputing of the projections of the context vector generated by the encoder in a encoder-decoder-architecture (i.e. in case of an RNN encoder the accumulated encoder hidden states).</p><p>By default attention scores are scaled according to Vaswani et al., 2017 <em>(Vaswani et al., Attention Is All You Need, CoRR, 2017)</em>.</p><p>All algorithms use soft attention.</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">Attn*Mechanism*(dec_units, enc_units; scale=true)
Attn*Mechanism*(units; scale=true)</code></pre><p>The one-argument version can be used, if encoder dimensions and decoder dimensions are the same.</p><p><strong>Common Signatures:</strong></p><pre><code class="nohighlight hljs">function (attn::AttentionMechanism)(h_t, h_enc; reset=false, mask=nothing)
function (attn::AttentionMechanism)(; reset=false)</code></pre><p><strong>Arguments:</strong></p><ul><li><code>h_t</code>:    decoder hidden state. If <span>$h_t$</span> is a vector, its length           equals the number of decoder units. If it is a matrix,           <span>$h_t$</span> includes the states for a minibatch of samples and has           the size [units, mb].</li><li><code>h_enc</code>:  encoder hidden states, 2d or 3d. If <span>$h_{enc}$</span> is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.</li><li><code>mask</code>:   optional mask (e.g. padding mask) for masking input steps           of dimensions [mb, steps]. Attentions factors for masked steps            will be set to 0.0.</li><li><code>reset=false</code>: If the keyword argument is set to <code>true</code>, projections of           the encoder states are computed. By default projections are           stored in the object and reused until the object is resetted.           For attention mechanisms that don&#39;t allow precomputation           the argument is ignored.</li></ul><p>The short form <code>(::AttentionMechanism)(reset=true)</code> can be used to reset the precomputed projections.</p><p><strong>Return values</strong></p><p>All functions return <code>c</code> and <code>α</code> where α is a matrix of size [mb,steps] with the attention factors for each step and minibatch. <code>c</code> is a matrix of size [units, mb] with the context vector for each sample of the minibatch, calculated as the α-weighted sum of all encoder hidden states <span>$h_{enc}$</span> for each minibatch.</p><p><strong>Attention Mechanisms:</strong></p><p>All attention mechanisms calculate attention factors α from scores derived from projections of the encoder hidden states:</p><p class="math-container">\[\alpha = \mathrm{softmax}(\mathrm{score}(h_{enc},h_{t}) \cdot 1/\sqrt{n}))\]</p><p>Attention mechanisms implemented:</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L3-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnBahdanau" href="#NNHelferlein.AttnBahdanau"><code>NNHelferlein.AttnBahdanau</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnBahdanau &lt;: AttentionMechanism</code></pre><p>Bahdanau-style (additive, concat) attention mechanism according to the paper:</p><p><em>D. Bahdanau, KH. Co, Y. Bengio, Neural Machine Translation by jointlylearning to align and translate, ICLR, 2015</em>.</p><p class="math-container">\[\mathrm{score}(h_{t},h_{enc}) = v_{a}^{\top}\cdot\tanh(W[h_{t},h_{enc}])\]</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnBahdanau(dec_units, enc_units; scale=true)
AttnBahdanau(units; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L79-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnLuong" href="#NNHelferlein.AttnLuong"><code>NNHelferlein.AttnLuong</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnLuong &lt;: AttentionMechanism</code></pre><p>Luong-style (multiplicative) attention mechanism according to the paper (referred as <em>General</em>-type attention): <em>M.-T. Luong, H. Pham, C.D. Manning, Effective Approaches to Attention-based Neural Machine Translation, CoRR, 2015</em>.</p><p class="math-container">\[\mathrm{score}(h_{t},h_{enc}) = h_{t}^{\top} W h_{enc}\]</p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnLuong(dec_units, enc_units; scale=true)
AttnLuong(units; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L168-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnDot" href="#NNHelferlein.AttnDot"><code>NNHelferlein.AttnDot</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnDot &lt;: AttentionMechanism</code></pre><p>Dot-product attention (without trainable parameters) according to the Luong, et al. (2015) paper.</p><p><span>$\mathrm{score}(h_{t},h_{enc}) = h_{t}^{\top} h_{enc}$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnDot(; scale=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L226-L236">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnLocation" href="#NNHelferlein.AttnLocation"><code>NNHelferlein.AttnLocation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnLocation &lt;: AttentionMechanism</code></pre><p>Location-based attention that only depends on the current decoder state <span>$h_t$</span> and not on the encoder states, according to the Luong, et al. (2015) paper.</p><p><span>$\mathrm{score}(h_{t}) = W h_{t}$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnLocation(len, dec_units; scale=true)</code></pre><ul><li><code>len</code>: maximum sequence length of the encoder to be considered       for attention. If the actual length of <span>$h_{enc}$</span> is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.</li><li><code>dec_units</code>: number of decoder units.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L269-L287">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.AttnInFeed" href="#NNHelferlein.AttnInFeed"><code>NNHelferlein.AttnInFeed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct AttnInFeed &lt;: AttentionMechanism</code></pre><p>Input-feeding attention that depends on the current decoder state <span>$h_t$</span> and the next input to the decoder <span>$i_{t+1}$</span>, according to the Luong, et al. (2015) paper.</p><p>Infeed attention provides a semantic attention that depends on the next input token.</p><p><span>$\mathrm{score}(h_{t}, i_{t+1}) = W_h h_{t} + W_i i_{t+1} = W [h_t, i_{t+1}]$</span></p><p><strong>Constructors:</strong></p><pre><code class="nohighlight hljs">AttnInFeed(len, dec_units, fan_in; scale=true)</code></pre><ul><li><code>len</code>: maximum sequence length of the encoder to be considered       for attention. If the actual length of <span>$h_{enc}$</span> is bigger as the       length of α, attention factors for the remaining states are set to       0.0. If the actual length of h_enc is smaller than α, only the matching       attention factors are applied.</li><li><code>dec_units</code>: number of decoder units.</li><li><code>fan_in</code>: size of the decoder input.</li></ul><p><strong>Signature:</strong></p><pre><code class="nohighlight hljs">function (attn::AttnInFeed)(h_t, inp, h_enc; mask=nothing)</code></pre><ul><li><code>h_t</code>:    decoder hidden state. If <span>$h_t$</span> is a vector, its length           equals the number of decoder units. If it is a matrix,           <span>$h_t$</span> includes the states for a minibatch of samples and has           the size [units, mb].</li><li><code>inp</code>: next decoder input <span>$i_{t+1}$</span>           (e.g. next embedded token of sequence)</li><li><code>h_enc</code>:  encoder hidden states, 2d or 3d. If <span>$h_{enc}$</span> is a           matrix [units, steps] with the hidden states of all encoder steps.           If 3d: [units, mb, steps] encoder states for all minibatches.</li><li><code>mask</code>:   Optional mask for input states of shape [mb, steps].</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/attn.jl#L342-L379">source</a></section></article><h2 id="Layers-for-transformers"><a class="docs-heading-anchor" href="#Layers-for-transformers">Layers for transformers</a><a id="Layers-for-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers-for-transformers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PositionalEncoding" href="#NNHelferlein.PositionalEncoding"><code>NNHelferlein.PositionalEncoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct PositionalEncoding &lt;: Layer</code></pre><p>Positional encoding layer. Only <em>sincos</em>-style (according to Vaswani, et al., NIPS 2017) is implemented.</p><p>The layer takes an array of any any number of dimensions (&gt;=2), calculates the Vaswani-2017-style positional encoding and adds the encoding to each plane of the array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/transformers.jl#L27-L36">source</a></section></article><h1 id="Data-providers"><a class="docs-heading-anchor" href="#Data-providers">Data providers</a><a id="Data-providers-1"></a><a class="docs-heading-anchor-permalink" href="#Data-providers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.DataLoader" href="#NNHelferlein.DataLoader"><code>NNHelferlein.DataLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type DataLoader</code></pre><p>Mother type for minibatch iterators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/types.jl#L2-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.PartialIterator" href="#NNHelferlein.PartialIterator"><code>NNHelferlein.PartialIterator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct PartialIterator &lt;: DataLoader</code></pre><p>The PartialIterator wraps any iterator and will only iterate the states specified in the list <code>indices</code>. </p><p><strong>Constuctors</strong></p><pre><code class="nohighlight hljs">PartialIterator(inner, indices; shuffle=true)</code></pre><p>Type of the states must match the states of the wrapped iterator <code>inner</code>. A <code>nothing</code> element may be  given to specify the first iterator element.</p><p>If <code>shuffle==true</code>, the list of indices are shuffled every time the PartialIterator is started.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/types.jl#L50-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.split_minibatches" href="#NNHelferlein.split_minibatches"><code>NNHelferlein.split_minibatches</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function split_minibatches(it, at=0.8; shuffle=true)</code></pre><p>Return 2 iterators od type <code>PartialIterator</code> which iterate only parts of the  states of the iterator it.  Be aware that the partial iterators will not contain copies of the data but instead forward the data provided by the iterator <code>it</code>.</p><p>The function can be used to split an iterator of minibatches into train-  and validation iterators, without copying any data. As the PartialIterator objects work with teh states of the inner iterator, it is important <em>not</em> to shuffle the inner iterator (in this case the  composition of the partial iterators would change!).</p><p><strong>Arguments:</strong></p><ul><li><code>it</code>: Iterator to be splitted. The list of allowed states is created by       performing a full iteration once.</li><li><code>at</code>: Split point. The first returned iterator will include the given        fraction (default: 80%) of the states.</li><li><code>shuffle</code>: If true, the elements are shuffled at each restart of the iterator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L268-L288">source</a></section></article><h2 id="Tabular-data"><a class="docs-heading-anchor" href="#Tabular-data">Tabular data</a><a id="Tabular-data-1"></a><a class="docs-heading-anchor-permalink" href="#Tabular-data" title="Permalink"></a></h2><p>Tabular data is normally provided in table form (csv, ods) row-wise, i.e. one sample per row. The helper functions can read the tables and generate Knet compatible iterators of minibatches.</p><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_read" href="#NNHelferlein.dataframe_read"><code>NNHelferlein.dataframe_read</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dataframe_read(fname)</code></pre><p>Read a data table from an CSV-file with one sample per row and return a DataFrame with the data. (ODS-support is removed because of PyCall compatibility issues of the OdsIO package).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_minibatches" href="#NNHelferlein.dataframe_minibatches"><code>NNHelferlein.dataframe_minibatches</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dataframe_minibatches(data::DataFrames.DataFrame; size=256, ignore=[], teaching=&quot;y&quot;, 
                      verbose=1, o...)</code></pre><p>Make Knet-conform minibatches of type <code>Knet.data</code> from a dataframe with one sample per row.</p><p><strong>Arguments:</strong></p><ul><li><code>ignore</code>: defines a list of column names to be ignored</li><li><code>teaching=&quot;y&quot;</code>: defines the column name with teaching input. Default is &quot;y&quot;.               <code>teaching</code> is handled differently, depending on its type:               If <code>Int</code>, the teaching input is interpreted as               class ids and directly used for training (this assumes that               the values range from 1..n). If type is a String, values are               interpreted as class labels and converted to numeric class IDs               by calling <code>mk_class_ids()</code>. The list of valid lables and their               order can be created by calling <code>mk_class_ids(data.y)[2]</code>.               If teaching is a scalar value, regression context is assumed,               and the value is used unchanged for training.</li><li><code>verbose=1</code>: if &gt; 0, a summary of how the dataframe is used is echoed.</li><li>other keyword arguments: all keyword arguments accepted by               <code>Knet.minibatch()</code> may be used.</li></ul><p>Allowed column definitions for <code>ignore</code> and <code>teaching</code> include names (as Strings), column names (as Symbols) or column indices (as Integer values).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L42-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dataframe_split" href="#NNHelferlein.dataframe_split"><code>NNHelferlein.dataframe_split</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function dataframe_split(df::DataFrames.DataFrame;
                         teaching=&quot;y&quot;, fr=0.2, balanced=true)</code></pre><p>Split data, organised row-wise in a DataFrame into train and valid sets.</p><p><strong>Arguments:</strong></p><ul><li><code>df</code>: data</li><li><code>teaching=&quot;y&quot;</code>: name or index of column with teaching input (y)</li><li><code>fr=0.2</code>: fraction of data to be used for validation</li><li><code>shuffle=true</code>: shuffle the rows of the dataframe.</li><li><code>balanced=true</code>: if <code>true</code>, result datasets will be balanced by oversampling.             Returned datasets will be bigger as expected             but include the same numbers of samples for each class.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L182-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_class_ids" href="#NNHelferlein.mk_class_ids"><code>NNHelferlein.mk_class_ids</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_class_ids(labels)</code></pre><p>Take a list with n class labels for n instances and return a list of n class-IDs (of type Int) and an array of lables with the array index of each label corresponds its ID.</p><p><strong>Arguments:</strong></p><ul><li><code>labels</code>: List of labels (typically Strings)</li></ul><p><strong>Result values:</strong></p><ul><li>array of class-IDs in the same order as the input</li><li>array of unique class-IDs ordered by their ID.</li></ul><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; labels = [&quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;blue&quot;]
7-element Array{String,1}:
 &quot;blue&quot;
 &quot;red&quot;
 &quot;red&quot;
 &quot;red&quot;
 &quot;green&quot;
 &quot;blue&quot;
 &quot;blue&quot;

julia&gt; mk_class_ids(labels)[1]
7-element Array{Int64,1}:
 1
 3
 3
 3
 2
 1
 1

 julia&gt; mk_class_ids(labels)[2]
3-element Array{String,1}:
 &quot;blue&quot;
 &quot;green&quot;
 &quot;red&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L128-L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.MBNoiser" href="#NNHelferlein.MBNoiser"><code>NNHelferlein.MBNoiser</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">type MBNoiser</code></pre><p>Iterator to wrap any Knet.Data iterator of minibatches in  order to add random noise.     Each value will be multiplied with a random value form  Gaussian noise with mean=1.0 and sd=sigma.</p><p><strong>Construtors:</strong></p><pre><code class="nohighlight hljs">MBNoiser(mbs::Knet.Data, σ=1.0)</code></pre><ul><li><code>mbs</code>: iteraor with minibatches</li><li><code>σ</code>: standard deviation for the Gaussian noise</li></ul><p><strong>Example:</strong></p><pre><code class="nohighlight hljs">trn = minibatch(x)
tb_train!(mdl, Adam, MBNoiser(trn, σ=0.1))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/dataframes.jl#L215-L235">source</a></section></article><h2 id="Image-data"><a class="docs-heading-anchor" href="#Image-data">Image data</a><a id="Image-data-1"></a><a class="docs-heading-anchor-permalink" href="#Image-data" title="Permalink"></a></h2><p>Images as data should be provided in directories with the directory names denoting the class labels. The helpers read from the root of a directory tree in which the first level of sub-dirs tell the class label. All images in the tree under a class label are read as instances of the respective class. The following tree will generate the classes <code>daisy</code>, <code>rose</code> and <code>tulip</code>:</p><pre><code class="nohighlight hljs">image_dir/
├── daisy
│   ├── 01
│   │   ├── 01
│   │   ├── 02
│   │   └── 03
│   ├── 02
│   │   ├── 01
│   │   └── 02
│   └── others
├── rose
│   ├── big
│   └── small
└── tulip</code></pre><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.ImageLoader" href="#NNHelferlein.ImageLoader"><code>NNHelferlein.ImageLoader</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct ImageLoader &lt;: DataLoader
    dir
    i_paths
    i_classes
    classes
    batchsize
    shuffle
    train
    aug_pipl
    pre_proc
    pre_load
    i_images
end</code></pre><p>Iterable image loader to provide minibatches of images as 4-d-arrays (x,y,rgb,mb).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L103-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_image_minibatch" href="#NNHelferlein.mk_image_minibatch"><code>NNHelferlein.mk_image_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_image_minibatch(dir, batchsize; split=false, fr=0.2,
                            balanced=false, shuffle=true, train=true,
                            pre_load=false,
                            aug_pipl=nothing, pre_proc=nothing)</code></pre><p>Return one or two iterable image-loader-objects that provides minibatches of images. For training each minibatch is a tupel <code>(x,y)</code> with x: 4-d-array with the minibatch of data and y: vector of class IDs as Int.</p><p><strong>Arguments:</strong></p><ul><li><code>dir</code>: base-directory of the image dataset. The first level of       sub-dirs are used as class names.</li><li><code>batchsize</code>: size of minibatches</li></ul><p><strong>Keyword arguments:</strong></p><ul><li><code>split</code>: return two iterators for training and validation</li><li><code>fr</code>: split fraction</li><li><code>balanced</code>: return balanced data (i.e. same number of instances       for all classes). Balancing is achieved via oversampling</li><li><code>shuffle</code>: if true, shuffle the images everytime the iterator       restarts</li><li><code>train</code>: if true, minibatches with (x,y) Tuples are provided,       if false only x (for prediction)</li><li><code>pre_load</code>: if <code>true</code> all images are loaded in advance;       otherwise images are loaded on demand durng training.       (option is <em>not implemented yet!</em>)</li><li><code>aug_pipl</code>: augmentation pipeline for Augmentor.jl. Augmentation       is performed before the pre_proc-function is applied</li><li><code>pre_proc</code>: function with preprocessing       and augmentation algoritms of type x = f(x). In contrast       to the augmentation that modifies images, is <code>pre_proc</code>       working on Arrays{Float32}.</li><li><code>pre_load=false</code>: read all images from disk once when populating the       loader (requires loads of memory, but speeds up training).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L7-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_class_labels" href="#NNHelferlein.get_class_labels"><code>NNHelferlein.get_class_labels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_class_labels(d::DataLoader)</code></pre><p>Extracts a list of class labels from a DataLoader.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L94-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.image2array" href="#NNHelferlein.image2array"><code>NNHelferlein.image2array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function image2array(img)</code></pre><p>Take an image and return a 3d-array for RGB and a 2d-array for grayscale images with the colour channels as last dimension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L309-L314">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2image" href="#NNHelferlein.array2image"><code>NNHelferlein.array2image</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function array2image(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return an array of RGB or of Gray as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L331-L336">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.array2RGB" href="#NNHelferlein.array2RGB"><code>NNHelferlein.array2RGB</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function array2RGB(arr)</code></pre><p>Take a 3d-array with colour channels as last dimension or a 2d-array and return always an array of RGB as Image.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/images.jl#L356-L361">source</a></section></article><h2 id="Text-data"><a class="docs-heading-anchor" href="#Text-data">Text data</a><a id="Text-data-1"></a><a class="docs-heading-anchor-permalink" href="#Text-data" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.WordTokenizer" href="#NNHelferlein.WordTokenizer"><code>NNHelferlein.WordTokenizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">mutable struct WordTokenizer
    len
    w2i
    i2w
end</code></pre><p>Create a word-based vocabulary: every unique word of a String or a list of Strings is assigned to a unique number. The created object includes a list of words (<code>i2w</code>, ordered by their numbers) and a dictionary <code>w2i</code> with the words as keys.</p><p>The constants <code>TOKEN_START, TOKEN_END, TOKEN_PAD</code> and <code>TOKEN_UNKOWN</code> are exported.</p><p><strong>Constructor:</strong></p><pre><code class="nohighlight hljs">function WordTokenizer(texts; len=nothing, add_ctls=true)</code></pre><p>With arguments:</p><ul><li><code>texts</code>: <code>AbstractArray</code> or iterable collection of <code>AbstractArray</code>s to be       analysed.</li><li><code>len=nothing</code>: maximum number of different words in the vocabulary.       Additional words in texts will be encoded as unknown. If <code>nothing</code>,       all words of the texts are included.</li><li><code>add_ctls=true</code>: if true, control words are added in front of the vocabulary       (extending the maximum length by 4): <code>&quot;&lt;start&gt;&quot;=&gt;1</code>, <code>&quot;&lt;end&gt;&quot;=&gt;2</code>,       <code>&quot;&lt;pad&gt;&quot;=&gt;3</code> and <code>&quot;&lt;unknown&gt;&quot;=&gt;4</code>.</li></ul><p><strong>Signatures:</strong></p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(w::T; split_words=false, add_ctls=false)
                            where {T &lt;: AbstractString}</code></pre><p>Encode a word and return the corresponding number in the vocabulary or the highest number (i.e. <code>&quot;&lt;unknown&gt;&quot;</code>) if the word is not in the vocabulary.</p><p>The encode-signature accepts the keyword arguments <code>split_words</code> and <code>add_ctls</code>. If <code>split_words==true</code>, the input is treated as a sentence and splitted into single words and an array of integer with the encoded sequence is returned. If <code>add_ctls==true</code> the sequence will be framed by <code>&lt;start&gt;</code> and <code>&lt;end&gt;</code> tokens.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(i::Integer)</code></pre><p>Decode a word by returning the word corresponding to <code>i</code> or &quot;&lt;unknown&gt;&quot; if the number is out of range of the vocabulary.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(s::AbstractArray{T}; add_ctls=false)
                           where {T &lt;: AbstractString}</code></pre><p>Called with an Array of Strings the tokeniser splits the strings into words and returns an Array of <code>Array{Integer}</code> with each of the input strings represented by a sequence of Integer values.</p><pre><code class="nohighlight hljs">function (t::WordTokenizer)(seq::AbstractArray{T}; add_ctls=false)
                                 where {T &lt;: Integer}</code></pre><p>Called with an Array of Integer values a single string  is returned with the decoded token-IDs as words (space-separated).</p><p><strong>Base Signatures:</strong></p><pre><code class="nohighlight hljs">    function length(t::WordTokenizer)</code></pre><p>Return the length of the vocab.</p><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; vocab = WordTokenizer([&quot;I love Julia&quot;, &quot;They love Python&quot;]);
Julia&gt; vocab(8)
&quot;Julia&quot;

julia&gt; vocab(&quot;love&quot;)
5

julia&gt; vocab.(split(&quot;I love Julia&quot;))
3-element Array{Int64,1}:
 5
 6
 8

julia&gt; vocab.i2w
9-element Array{String,1}:
 &quot;&lt;start&gt;&quot;
 &quot;&lt;end&gt;&quot;
 &quot;&lt;pad&gt;&quot;
 &quot;&lt;unknown&gt;&quot;
 &quot;love&quot;
 &quot;I&quot;
 &quot;They&quot;
 &quot;Julia&quot;
 &quot;Python&quot;

julia&gt; vocab.w2i
Dict{String,Int64} with 9 entries:
  &quot;I&quot;         =&gt; 6
  &quot;&lt;end&gt;&quot;     =&gt; 2
  &quot;&lt;pad&gt;&quot;     =&gt; 3
  &quot;They&quot;      =&gt; 7
  &quot;Julia&quot;     =&gt; 8
  &quot;love&quot;      =&gt; 5
  &quot;Python&quot;    =&gt; 9
  &quot;&lt;start&gt;&quot;   =&gt; 1
  &quot;&lt;unknown&gt;&quot; =&gt; 4

julia&gt; vocab.([7,5,8])
3-element Array{String,1}:
 &quot;They&quot;
 &quot;love&quot;
 &quot;Julia

julia&gt; vocab.(&quot;I love Scala&quot;, split_words=true)
3-element Array{Int64,1}:
 6
 5
 4

julia&gt; vocab.([6,5,4])
3-element Array{String,1}:
 &quot;I&quot;
 &quot;love&quot;
 &quot;&lt;unknown&gt;&quot;

julia&gt; vocab(&quot;I love Python&quot;, split_words=true, add_ctls=true)
5-element Array{Int64,1}:
 1
 6
 5
 9
 2

julia&gt; vocab([&quot;They love Julia&quot;, &quot;I love Julia&quot;])
2-element Array{Array{Int64,1},1}:
 [7, 5, 8]
 [6, 5, 8]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/texts.jl#L10-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_tatoeba_corpus" href="#NNHelferlein.get_tatoeba_corpus"><code>NNHelferlein.get_tatoeba_corpus</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_tatoeba_corpus(lang; force=false,
            url=&quot;https://www.manythings.org/anki/&quot;)</code></pre><p>Download and read a bilingual text corpus from Tatoeba (privided) by ManyThings (https://www.manythings.org). All corpi are English-<em>Language</em>-pairs with different size and quality. Considerable languages include:</p><ul><li><code>fra</code>: French-English, 180 000 sentences</li><li><code>deu</code>: German-English, 227 000 sentences</li><li><code>heb</code>: Hebrew-English, 126 000 sentences</li><li><code>por</code>: Portuguese-English, 170 000 sentences</li><li><code>tur</code>: Turkish-English, 514 000 sentences</li></ul><p>The function returns two lists with corresponding sentences in both languages. Sentences are are <em>not</em> processed/normalised/cleaned, but exactly as provided by Tatoeba.</p><p>The data is stored in the package directory and only downloaded once.</p><p><strong>Arguments:</strong></p><ul><li><code>lang</code>: languagecode</li><li><code>force=false</code>: if <code>true</code>, the corpus is downloaded even if       a data file is already saved.</li><li><code>url</code>: base url of ManyThings.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/texts.jl#L284-L309">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.sequence_minibatch" href="#NNHelferlein.sequence_minibatch"><code>NNHelferlein.sequence_minibatch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function sequence_minibatch(x, [y], batchsize; 
                            pad=NNHelferlein.TOKEN_PAD, 
                            seq2seq=true, pad_y=pad,
                            x_padding=false,
                            shuffle=true, partial=false)</code></pre><p>Return an iterator of type <code>DataLoader</code> with (x,y) sequence minibatches from two lists of sequences.</p><p>All sequences within a minibatch in x and y are brought to the same length by padding with the token provided as <code>pad</code>.</p><p>The sequences are sorted by length before building minibatches in order to  reduce padding (i.e. sequences of similar length are combined to a minibatch). If the same sequence length is needed for all minibatches, the sequences must be truncted or padded before call of <code>sequence_minibatch()</code>  (see functions <code>truncate_seqence()</code> and <code>pad_sequence()</code>).</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: List of sequences of <code>Int</code></li><li><code>y</code>: List of sequences of <code>Int</code> or list of target values (i.e. teaching input)</li><li><code>batchsize</code>: size of minibatches</li><li><code>pad=NNHelferlein.PAD_TOKEN</code>,</li><li><code>pad_y=x</code>: token, used for padding. The token must be compatible       with the type of the sequence elements. If <code>pad_y</code> is omitted, it is set        equal to pad_x.</li><li><code>seq2seq=true</code>: if <code>true</code> and <code>y</code> is provided, sequence-to-sequence minibatches are        created. Otherwise <code>y</code> is treated as scalar teaching input.</li><li><code>shuffle=true</code>: The minibatches are shuffled as last step. If <code>false</code> the minibatches        with short sequences will be at the beginning of the dataset.</li><li><code>partial=false</code>: If <code>true</code>, a partial minibatch will be created if necessray to        include all input data.</li><li><code>x_padding=false</code>: if <code>true</code>, pad sequences in x to make minibatches of the demanded size,        even if there are not       enougth sequences of the same length in x.       If <code>false</code>, partial minibatches are built (if partial == <code>true</code>) or remaining        sequneces are skipped (if partial == <code>false</code>).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/texts.jl#L473-L512">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.pad_sequence" href="#NNHelferlein.pad_sequence"><code>NNHelferlein.pad_sequence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function pad_sequence(s, len; token=NNHelferlein.TOKEN_PAD)</code></pre><p>Stretch a sequence to length <code>len</code> by adding the padding token.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/texts.jl#L608-L612">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.truncate_sequence" href="#NNHelferlein.truncate_sequence"><code>NNHelferlein.truncate_sequence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function truncate_sequence(s, len; end_token=nothing)</code></pre><p>Truncate a sequence to the length <code>len</code>.  If not <code>isnothing(end_token)</code>, the last token of the sequenceis  overwritten by the token.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/texts.jl#L622-L628">source</a></section></article><h1 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.tb_train!" href="#NNHelferlein.tb_train!"><code>NNHelferlein.tb_train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function tb_train!(mdl, opti, trn, vld=nothing; epochs=1, split=nothing,
                  lr_decay=nothing, lrd_steps=5, lrd_linear=false,
                  l2=0.0,
                  eval_size=0.2, eval_freq=1,
                  acc_fun=nothing,
                  mb_loss_freq=100,
                  checkpoints=nothing, cp_dir=&quot;checkpoints&quot;,
                  tb_dir=&quot;logs&quot;, tb_name=&quot;run&quot;,
                  tb_text=&quot;&quot;&quot;Description of tb_train!() run.&quot;&quot;&quot;,
                  opti_args...)</code></pre><p>Train function with TensorBoard integration. TB logs are written with the TensorBoardLogger.jl package. The model is updated (in-place) and the trained model is returned.</p><p><strong>Arguments:</strong></p><ul><li><code>mdl</code>: model; i.e. forward-function for the net</li><li><code>opti</code>: Knet-stype optimiser type</li><li><code>trn</code>: training data; iterator to provide (x,y)-tuples with       minibatches</li><li><code>vld</code>: validation data; iterator to provide (x,y)-tuples with       minibatches. Set to <code>nothing</code>, if not defined.</li></ul><p><strong>Keyword arguments:</strong></p><p><strong>Optimiser:</strong></p><ul><li><code>epochs=1</code>: number of epochs to train</li><li><code>lr_decay=nothing</code>: do a leraning rate decay if not <code>nothing</code>:       the value given is the final learning rate after <code>lrd_steps</code>       steps of decay (<code>lr_decay</code> may be bigger than <code>lr</code>; in this case       the leraning rate is increased).        <code>lr_decay</code> is only applied if both start learning rate       <code>lr</code> and final learning rate <code>lr_decay</code> are defined explicitly.       Example: <code>lr=0.01, lr_decay=0.001</code> will reduce the lr from       0.01 to 0.001 during the training (by default in 5 steps).</li><li><code>lrd_steps=5</code>: number of learning rate decay steps. Default is <code>5</code>, i.e.       modify the lr 4 times during the training (resulting in 5 different        leraning rates).</li><li><code>lrd_linear=false</code>: type of learning rate decay;       If <code>false</code>, lr is modified       by a constant factor (e.g. 0.9) resulting in an exponential decay.       If <code>true</code>, lr is modified by the same step size, i.e. linearly.</li><li><code>l2=0.0</code>: L2 regularisation; implemented as weight decay per       parameter</li><li><code>opti_args...</code>: optional keyword arguments for the optimiser can be specified       (i.e. <code>lr</code>, <code>gamma</code>, ...).</li></ul><p><strong>Model evaluation:</strong></p><ul><li><code>split=nothing</code>: if no validation data is specified and split is a        fraction (between 0.0 and 1.0), the training dataset is splitted at the       specified point (e.g.: if <code>split=0.8</code>, 80% of the minibatches are used        for training and 20% for validation).</li><li><code>eval_size=0.2</code>: fraction of validation data to be used for calculating       loss and accuracy for train and validation data during training.</li><li><code>eval_freq=1</code>: frequency of evaluation; default=1 means evaluation is       calculated after each epoch. With eval_freq=10 eveluation is       calculated 10 times per epoch.</li><li><code>acc_fun=nothing</code>: function to calculate accuracy. The function       must implement the following signature: <code>fun(model; data)</code> where       data is an iterator that provides (x,y)-tuples of minibatches.       For classification tasks, <code>accuracy</code> from the Knet package is       a good choice. For regression a correlation or mean error       may be preferred.</li><li><code>mb_loss_freq=100</code>: frequency of training loss reporting. default=100       means that 100 loss-values per epoch will be logged to TensorBoard.       If mb<em>loss</em>freq is greater then the number of minibatches,       loss is logged for each minibatch.</li><li><code>checkpoints=nothing</code>: frequency of model checkpoints written to disk.       Default is <code>nothing</code>, i.e. no checkpoints are written.       To write the model after each epoch with       name <code>model</code> use cp<em>epoch=1; to write every second epochs cp</em>epoch=2,        etc.</li><li><code>cp_dir=&quot;checkpoints&quot;</code>: directory for checkpoints</li></ul><p><strong>TensorBoard:</strong></p><p>TensorBoard log-directory is created from 3 parts: <code>tb_dir/tb_name/&lt;current date time&gt;</code>.</p><ul><li><code>tb_dir=&quot;logs&quot;</code>: root directory for tensorborad logs.</li><li><code>tb_name=&quot;run&quot;</code>: name of training run. <code>tb_name</code> will be used as       directory name and should not include whitespace</li><li><code>tb_text</code>:  description       to be included in the TensorBoard log as <em>text</em> log.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/train.jl#L1-L84">source</a></section></article><h1 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict" href="#NNHelferlein.predict"><code>NNHelferlein.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict(mdl, x; softmax=false)</code></pre><p>Return the prediction for x.</p><p><strong>Arguments:</strong></p><ul><li><code>mdl</code>: executable network model</li><li><code>x</code>: iterator providing minibatches       of input data</li><li><code>softmax</code>: if true or if model is of type <code>Classifier</code> the predicted       softmax probabilities are returned instead of raw       activations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/train.jl#L441-L453">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_top5" href="#NNHelferlein.predict_top5"><code>NNHelferlein.predict_top5</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict_top5(mdl, x; top_n=5, classes=nothing)</code></pre><p>Run the model <code>mdl</code> for data in <code>x</code> and print the top 5 predictions as softmax probabilities.</p><p><strong>Arguments:</strong></p><ul><li><code>top_n</code>: print top <em>n</em> hits</li><li><code>classes</code>: optional list of human readable class labels.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/train.jl#L410-L419">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.hamming_dist" href="#NNHelferlein.hamming_dist"><code>NNHelferlein.hamming_dist</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function hamming_dist(p, t; accuracy=false, 
                            ignore_ctls=false, vocab=nothing, 
                            start=nothing, stop=nothing, pad=nothing, unk=nothing)


function hamming_acc(p, t; o...)

function hamming_acc(mdl; data=data, o...)</code></pre><p>Return the Hamming distance between two sequences or two minibatches of sequences. Predicted sequences <code>p</code> and teaching input sequences <code>t</code> may be of different length but the number of sequences in the minibatch must be the same.</p><p><strong>Arguments:</strong></p><ul><li><code>p</code>, <code>t</code>: n-dimensional arrays of type <code>Int</code> with predictions       and teaching input for a minibatch of sequences.       Shape of the arrays must be identical except of the first dimension       (i.e. the sequence length) that may differ between <code>p</code> and <code>t</code>.</li><li><code>accuracy=false</code>: if <code>false</code>, the mean Hamming distance in the minibatch       is returned (i.e. the average number of differences in the sequences).       If <code>true</code>, the accuracy is returned       for all not padded positions in a range (0.0 - 1.0).</li><li><code>ignore_ctls=false</code>: a vocab is used to replace all &#39;&lt;start&gt;, &lt;end&gt;, &lt;unknwon&gt;, &lt;pad&gt;&#39;       tokens by <code>&lt;pad&gt;</code>. If true, padding and other control tokens are treated as       normal codes and are not ignored.</li><li><code>vocab=nothing</code>: target laguage vocabulary of type <code>NNHelferlein.WordTokenizer</code>.       If defined,       the padding token of <code>vocab</code> is used to mask all control tokens in the       sequences (i.e. &#39;&lt;start&gt;, &lt;end&gt;, &lt;unknwon&gt;, &lt;pad&gt;&#39;).</li><li><code>start, stop, pad, unk</code>: may be used to define individual control tokens.       default is <code>nothing</code>.</li></ul><p><strong>Details:</strong></p><p>The function <code>hamming_acc()</code> is a shortcut to return the accuracy instead of the distance. The signature <code>hamming_acc(mdl; data=data; o...)</code> is for compatibility with acc functions called by train.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/acc.jl#L167-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.peak_finder_acc" href="#NNHelferlein.peak_finder_acc"><code>NNHelferlein.peak_finder_acc</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function peak_finder_acc(p, t; ret=:f1, verbose=0, 
                         tolerance=1, limit=0.5

function peak_finder_acc(mdl; data=data, o...)</code></pre><p>Calculate an accuracy-like measure for data series consisting  mainly of zeros and rare peaks. The function counts the number of peaks in <code>y</code> detected by <code>p</code>  (<em>true positives</em>), peaks not detected (<em>false negatives</em>)  and the nnumber of peaks in <code>p</code> not present in <code>y</code>  (<em>false positives</em>).</p><p>It is assumed that peaks in <code>y</code> are marked by a single value higher as the limit (typically 1.0). Peaks in <code>p</code> may be  broader; and are defined as local maxima with a value above the limit. If the tolerance ist set to &gt;0, it may happen that the peaks at the first  or last step are not evaluated (because evaluation stopss at  <code>end-tolerance</code>).</p><p>If requested, <em>f1</em>, <em>G-mean</em> and <em>intersection over union</em>  are calulated from the raw values .</p><p><strong>Arguments:</strong></p><ul><li><code>p</code>, <code>t</code>: Predictions and teaching input (i.e. <code>y</code>) are mini-batches of           1-d series of data. The sequence must be in the 1st dimension           (column). All other dims are treated as separate windows           of length size(p/t,1).</li><li><code>ret</code>: return value as <code>Symbol</code>; one of        <code>:peaks</code>, <code>:recall</code>, <code>:precision</code>, <code>:miss_rate</code>, <code>:f1</code>,       <code>:g_mean</code>, <code>:iou</code> or <code>:all</code>.       If <code>:all</code> a named tuple is returned.</li><li><code>vervose=0</code>: if <code>0</code>, no additional output is generated;       if <code>1</code>, composite measures are printed to stdout;       if <code>2</code>, all raw counts are printed.</li><li><code>tolerance=1</code>: peak finder tolerance: The peak is defined as <em>correct</em>       if it is detected within the tolerance.</li><li><code>limit=0.5</code>: Only maxima with values above the limit are considered.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/acc.jl#L8-L48">source</a></section></article><h1 id="ImageNet-tools"><a class="docs-heading-anchor" href="#ImageNet-tools">ImageNet tools</a><a id="ImageNet-tools-1"></a><a class="docs-heading-anchor-permalink" href="#ImageNet-tools" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.preproc_imagenet" href="#NNHelferlein.preproc_imagenet"><code>NNHelferlein.preproc_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function preproc_imagenet(img)</code></pre><p>Image preprocessing for pre-trained ImageNet examples. Preprocessing includes</p><ul><li>bring RGB colour values into a range 0-255</li><li>standardise of colour values by substracting mean colour values   (103.939, 116.779, 123.68) from RGB</li><li>changing colour channel sequence from RGB to BGR</li></ul><p>Resize is <strong>not</strong> done, because this may be part of the augmentation pipeline.</p><p><strong>Examples:</strong></p><p>The function can be used with the image loader; for prediction with a trained model as:</p><pre><code class="language-julia hljs">pipl = CropRatio(ratio=1.0) |&gt; Resize(224,224)
images = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    shuffle=false, train=false,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre><p>And for training something like:</p><pre><code class="language-julia hljs">pipl = Either(1=&gt;FlipX(), 1=&gt;FlipY(), 2=&gt;NoOp()) |&gt;
       Rotate(-5:5) |&gt;
       ShearX(-5:5) * ShearY(-5:5) |&gt;
       RCropSize(224,224)

dtrn, dvld = mk_image_minibatch(&quot;./example_pics&quot;, 16;
                    split=true, fr=0.2, balanced=false,
                    shuffle=true, train=true,
                    aug_pipl=pipl,
                    pre_proc=preproc_imagenet)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/imagenet.jl#L2-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.predict_imagenet" href="#NNHelferlein.predict_imagenet"><code>NNHelferlein.predict_imagenet</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function predict_imagenet(mdl, x; top_n=5)</code></pre><p>Predict the ImageNet-class of images from the predefined list of class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/imagenet.jl#L76-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.get_imagenet_classes" href="#NNHelferlein.get_imagenet_classes"><code>NNHelferlein.get_imagenet_classes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function get_imagenet_classes()</code></pre><p>Return a list of all 1000 ImageNet class labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/imagenet.jl#L53-L57">source</a></section></article><h1 id="Other-utils"><a class="docs-heading-anchor" href="#Other-utils">Other utils</a><a id="Other-utils-1"></a><a class="docs-heading-anchor-permalink" href="#Other-utils" title="Permalink"></a></h1><h2 id="Utils-for-transformers"><a class="docs-heading-anchor" href="#Utils-for-transformers">Utils for transformers</a><a id="Utils-for-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-transformers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.positional_encoding_sincos" href="#NNHelferlein.positional_encoding_sincos"><code>NNHelferlein.positional_encoding_sincos</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function positional_encoding_sincos(n_embed, n_seq)</code></pre><p>Calculate and return a matrix of size <code>[n_embed, n_seq]</code> of positional encoding values following the sin and cos style in the paper <em>Vaswani, A. et al.; Attention Is All You Need; 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017.</em></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/transformers.jl#L7-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_padding_mask" href="#NNHelferlein.mk_padding_mask"><code>NNHelferlein.mk_padding_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_padding_mask(x; pad=TOKEN_PAD, add_dims=false)</code></pre><p>Make a padding mask; i.e. return an Array of type <code>KnetArray{Float32}</code> (or <code>Array{Float32}</code>) similar to <code>x</code> but with two additional dimension of size 1 in the middle (this will represent the 2nd seq_len and the number of heads) in multi-head attention and the value <code>1.0</code> at each position where <code>x</code> is <code>pad</code> and <code>0.0</code> otherwise.</p><p>The function can be used for creating padding masks for attention mechanisms.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: Array of sequences (typically a matrix with n<em>cols sequences   of length n</em>rows)</li><li><code>pad</code>: value for the token to be masked</li><li><code>add_dims</code>: if <code>true</code>, 2 additional dimensions are inserted to    return a 4-D-array as needed for transformer architectures. Otherwise   the size of teh returned array is similar to x.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/transformers.jl#L51-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.mk_peek_ahead_mask" href="#NNHelferlein.mk_peek_ahead_mask"><code>NNHelferlein.mk_peek_ahead_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function mk_peek_ahead_mask(x; dim=1)</code></pre><p>Return a matrix of size <code>[n_seq, n_seq]</code> filled with 1.0 and the <em>uppper triangle</em> set to 0.0. Type is <code>KnetArray{Float32}</code> in GPU context, <code>Array{Float32}</code> otherwise. The matrix can be used as peek-ahead mask in transformers.</p><p><code>dim=1</code> specifies the dimension in which the sequence length is represented. For un-embedded data this is normally <code>1</code>, i.e. the shape of x is [n<em>seq, n</em>mb]. After embedding the shape probably is [depth, n<em>seq, n</em>mb].</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/transformers.jl#L82-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.dot_prod_attn" href="#NNHelferlein.dot_prod_attn"><code>NNHelferlein.dot_prod_attn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function dot_prod_attn(q, k, v; mask=nothing)</code></pre><p>Generic scaled dot product attention following the paper of Vaswani et al., (2017), <em>Attention Is All You Need</em>.</p><p><strong>Arguments:</strong></p><ul><li><code>q</code>: query of size <code>[depth, n_seq_q, ...]</code></li><li><code>k</code>: key of size <code>[depth, n_seq_v, ...]</code></li><li><code>v</code>: value of size <code>[depth, n_seq_v, ...]</code></li><li><code>mask</code>: mask for attention factors may have different shapes but must be       broadcastable for addition to the scores tensor (which as the same size as       alpha <code>[n_seq_v, n_seq_q, ...]</code>). In transformer context typical masks are one of:       padding mask of size <code>[n_seq_v, ...]</code> or a peek-ahead mask of size <code>[n_seq_v, n_seq_v]</code>       (which is only possible in case of self-attention when all seqencee lengths       are identical).</li></ul><p><code>q, k, v</code> must have matching leading dimensions (i.e. same depth or embedding). <code>k</code> and <code>v</code> must have the same sequence length.</p><p><strong>Return values:</strong></p><ul><li><code>c</code>: context as alpha-weighted sum of values with size [depth, n<em>seq</em>v, ...]</li><li><code>alpha</code>: attention factors of size [n<em>seq</em>v, n<em>seq</em>q, ...]</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/transformers.jl#L103-L126">source</a></section></article><h2 id="Utils-for-array-manipulation"><a class="docs-heading-anchor" href="#Utils-for-array-manipulation">Utils for array manipulation</a><a id="Utils-for-array-manipulation-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-array-manipulation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.crop_array" href="#NNHelferlein.crop_array"><code>NNHelferlein.crop_array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function crop_array(x, crop_sizes)</code></pre><p>Crop a n-dimensional array to the given size. Cropping is always centered (i.e. a margin is removed).</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: n-dim AbstractArray</li><li><code>crop_sizes</code>: Tuple of target sizes to which the array is cropped.       Allowed values are Int or <code>:</code>. If <code>crop_sizes</code> defines less       dims as x has, the remaining dims will not be cropped (assuming <code>:</code>).       If a demanded crop size is bigger as the actual size of x,       it is ignored.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/util.jl#L23-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.blowup_array" href="#NNHelferlein.blowup_array"><code>NNHelferlein.blowup_array</code></a> — <span class="docstring-category">Function</span></header><section><div><p>function blowup_array(x, n)</p><p>Blow up an array <code>x</code> with an additional dimension and repeat the content of the array <code>n</code> times.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: Array of any dimension</li><li><code>n</code>: number of repeats. ´n=1´ will return an</li></ul><p>array with an additional dimension of size 1.</p><p><strong>Examples:</strong></p><pre><code class="language-Julia hljs">julia&gt; x = [1,2,3,4]; blowup_array(x, 3)
4×3 Array{Int64,2}:
 1  1  1
 2  2  2
 3  3  3
 4  4  4

julia&gt; x = [1 2; 3 4]; blowup_array(x, 3)
2×2×3 Array{Int64,3}:
[:, :, 1] =
 1  2
 3  4

[:, :, 2] =
 1  2
 3  4

[:, :, 3] =
 1  2
 3  4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/util.jl#L165-L201">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.recycle_array" href="#NNHelferlein.recycle_array"><code>NNHelferlein.recycle_array</code></a> — <span class="docstring-category">Function</span></header><section><div><p>function recycle_array(x, n; dims=dims(x))</p><p>Recycle an array <code>x</code> along the specified dimension  (default the last dimension) and repeat the content of the array <code>n</code> times. The number of dims stays unchanged, but the array valueas are repeated <code>n</code> times.</p><p><strong>Arguments:</strong></p><ul><li><code>x</code>: Array of any dimension</li><li><code>n</code>: number of repeats. ´n=1´ will return an unchanged       array</li><li><code>dims</code>: dimension to be repeated.</li></ul><p><strong>Examples:</strong></p><pre><code class="language-Julia hljs">julia&gt; recycle_array([1,2],3)
6-element Array{Int64,1}:
 1
 2
 1
 2
 1
 2

julia&gt; x = [1 2; 3 4]
2×2 Array{Int64,2}:
 1  2
 3  4

julia&gt; recycle_array(x,3)
2×6 Array{Int64,2}:
 1  2  1  2  1  2
 3  4  3  4  3  4

julia&gt; recycle_array([1 2 3],3, dims=1)
3x3 Array{Int64,2}:
 1 2 3
 1 2 3
 1 2 3</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/util.jl#L215-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.de_embed" href="#NNHelferlein.de_embed"><code>NNHelferlein.de_embed</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function de_embed(x)</code></pre><p>Replace the maximum of the first dimension of an n-dimensional array by its index (aka argmax()). The returned array has the first dimension with size 1.</p><p><strong>Examples:</strong></p><pre><code class="language-Julia hljs">x = [1,1,3,4,2]
de_embed(x)
&gt; 4

x = [1 1 1
     2 1 1
     1 2 1
     1 1 2]
de_embed(x)
&gt; [2 3 4]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/util.jl#L283-L303">source</a></section></article><h2 id="Utils-for-fixing-types-in-GPU-context"><a class="docs-heading-anchor" href="#Utils-for-fixing-types-in-GPU-context">Utils for fixing types in GPU context</a><a id="Utils-for-fixing-types-in-GPU-context-1"></a><a class="docs-heading-anchor-permalink" href="#Utils-for-fixing-types-in-GPU-context" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNHelferlein.init0" href="#NNHelferlein.init0"><code>NNHelferlein.init0</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function init0(siz...)</code></pre><p>Initialise a vector or array of size <code>siz</code> with zeros. If a GPU is detected type of the returned value is <code>KnetArray{Float32}</code>, otherwise <code>Array{Float32}</code>.</p><p><strong>Examples:</strong></p><pre><code class="nohighlight hljs">julia&gt; init0(2,10)
2×10 Array{Float32,2}:
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0

 julia&gt; init0(0,10)
 0×10 Array{Float32,2}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/andreasdominik/NNHelferlein.jl/blob/de4594791d9900345ce65e783d102035c8f275f4/src/util.jl#L61-L78">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>convert2KnetArray</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>emptyKnetArray</code>. Check Documenter&#39;s build log for details.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../license/">License »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.14 on <span class="colophon-date" title="Saturday 5 March 2022 11:59">Saturday 5 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
